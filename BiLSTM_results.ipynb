{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "183d3988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_reader import DataReader\n",
    "import datasets\n",
    "from tweet_to_vec import TweetToVec\n",
    "import utils\n",
    "from BiLSTM import BiLSTMModel\n",
    "from evaluate import evaluate1\n",
    "from evaluate import evaluate2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1335402",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = DataReader('nlkt')\n",
    "dr.read_dataset(datasets.binary_classes, False)\n",
    "dr.read_dataset(datasets.ternary_classes, False)\n",
    "embeddings = dr.read_embeddings('embeddings/kraby.txt', False)\n",
    "\n",
    "binary_dataset = dr.get_dataset('binary')\n",
    "binary_dataset = utils.extract_validation_from_training(binary_dataset)\n",
    "\n",
    "ternary_dataset = dr.get_dataset('ternary')\n",
    "ternary_dataset = utils.extract_validation_from_training(ternary_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e161fe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataset, epochs, method, L, size, num_layers, dropout, prediction_method, equalize_training_classes, learning_rate, number_of_output_classes):\n",
    "    batch_size = 32\n",
    "    t2v = TweetToVec(embeddings, method, L)\n",
    "\n",
    "    if equalize_training_classes:\n",
    "        dataset = utils.equalize_training_classes(dataset)\n",
    "    vectorized_dataset = t2v.vectorize_dataset(dataset)\n",
    "    def gen_batched():\n",
    "        batched_dataset = t2v.batch_dataset(vectorized_dataset, batch_size, True, True)\n",
    "        return batched_dataset['training tweets'], batched_dataset['training tags']\n",
    "\n",
    "    model = BiLSTMModel(size=size, embedding_dim=100,num_layers=num_layers, dropout=dropout, number_of_output_classes=number_of_output_classes, L=L, prediction_method=prediction_method, learning_rate=learning_rate)\n",
    "\n",
    "    batched = t2v.batch_dataset(vectorized_dataset, batch_size, True, False)\n",
    "    tweets = batched['validation tweets']\n",
    "    tags = batched['validation tags']\n",
    "    current_best_score = 0\n",
    "    best_model_path = \"\"\n",
    "    params_string = f'_number_of_output_classes_{number_of_output_classes}_method_{method}_L_{L}_size_{size}_num_layers_{num_layers}_dropout_{dropout}_prediction_method_{prediction_method}_equalize_training_classes_{equalize_training_classes}_learnig_rate_{learning_rate}'\n",
    "    \n",
    "    scores = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        model.train_model(gen_batched, 1)\n",
    "        model.eval()\n",
    "        predictions = model.predict(tweets)\n",
    "        score = 0\n",
    "        if number_of_output_classes == 2:\n",
    "            score = evaluate1(predictions, tags)['balancedf']\n",
    "            print(\" balancedf score on valid =\", score)\n",
    "        else:\n",
    "            score = evaluate2(predictions, tags)['microAverageFscore']\n",
    "            print(\" microAverageFscore score on valid =\", score)  \n",
    "        scores.append(score)  \n",
    "        if score > current_best_score:\n",
    "            if len(best_model_path) > 0:\n",
    "                os.remove(best_model_path)\n",
    "            current_best_score = score\n",
    "            str_score = str(score * 100)\n",
    "            str_score = str_score[:min(len(str_score), 5)]\n",
    "            best_model_path = 'models/LSTM_' + str_score + params_string + '.model'\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "    \n",
    "\n",
    "    plt.plot(scores)\n",
    "    plt.ylabel('scores on validation over epochs')\n",
    "    plt.show()\n",
    "    print(\"Best score = \", current_best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a03067dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_test(filename, dataset, size, num_layers, dropout, L, prediction_method, method, number_of_output_classes):\n",
    "    model = BiLSTMModel(size=size, embedding_dim=100,num_layers=num_layers, dropout=dropout, number_of_output_classes=number_of_output_classes, L=L, prediction_method=prediction_method, learning_rate=0.0001)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    model.eval()\n",
    "\n",
    "    t2v = TweetToVec(embeddings, method, L)\n",
    "    \n",
    "    vectorized_dataset = t2v.vectorize_dataset(dataset)\n",
    "\n",
    "    batched = t2v.batch_dataset(vectorized_dataset, 1, True, False)\n",
    "    tweets = batched['test tweets']\n",
    "    tags = batched['test tags']\n",
    "\n",
    "    predictions = model.predict(tweets)\n",
    "    if number_of_output_classes == 2:\n",
    "        score = evaluate1(predictions, tags)['balancedf']\n",
    "        print(\" balancedf score on test =\", score)\n",
    "    else:\n",
    "        score = evaluate2(predictions, tags)['microAverageFscore']\n",
    "        print(\" microAverageFscore score on test =\", score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fd95eb8",
   "metadata": {},
   "source": [
    "## Hyperparameters turning of LSTM on binary problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7e2103d",
   "metadata": {},
   "source": [
    "### First round of hyperparameters turning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74c4b536",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(dataset=binary_dataset, \n",
    "     epochs=15,\n",
    "     method='fixed_length_2d', \n",
    "     L=30, \n",
    "     size=128, \n",
    "     num_layers=2, \n",
    "     dropout=0.2, \n",
    "     prediction_method='sum',\n",
    "     equalize_training_classes=True, \n",
    "     learning_rate=0.0001,\n",
    "     number_of_output_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adcc4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(dataset=binary_dataset, \n",
    "     epochs=15,\n",
    "     method='fixed_length_2d',\n",
    "     L=30, \n",
    "     size=64, \n",
    "     num_layers=1, \n",
    "     dropout=0.2, \n",
    "     prediction_method='max',\n",
    "     equalize_training_classes=True, \n",
    "     learning_rate=0.0001, \n",
    "     number_of_output_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ecc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(dataset=binary_dataset, \n",
    "     epochs=15,\n",
    "     method='fixed_length_2d', \n",
    "     L=30, \n",
    "     size=128, \n",
    "     num_layers=2, \n",
    "     dropout=0.2, \n",
    "     prediction_method='sum',\n",
    "     equalize_training_classes=True, \n",
    "     learning_rate=0.001, \n",
    "     number_of_output_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fff6091",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(dataset=binary_dataset, \n",
    "     epochs=15,\n",
    "     method='fixed_length_2d', \n",
    "     L=30, \n",
    "     size=256,\n",
    "     num_layers=2, \n",
    "     dropout=0.5, \n",
    "     prediction_method='max',\n",
    "     equalize_training_classes=True, \n",
    "     learning_rate=0.0001, \n",
    "     number_of_output_classes=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef5cbdfd",
   "metadata": {},
   "source": [
    "### Second iterations round (small changed of the best from the previous round)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8335b50",
   "metadata": {},
   "source": [
    "### Evaluation of the best LSTM model on binary problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be00785b",
   "metadata": {},
   "source": [
    "## Hyperparameters turning of LSTM on ternary problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4724f59",
   "metadata": {},
   "source": [
    "### First tryouts based on best models from the binary problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf74273f",
   "metadata": {},
   "source": [
    "### Evaluation of the best LSTM model on ternary problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f97c87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
