{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1926ec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_reader import *\n",
    "from abstract_classifier import AbstractClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ee3cd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10041/10041 [00:00<00:00, 10659.20it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10041/10041 [00:00<00:00, 3403507.88it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10041/10041 [00:00<00:00, 10772.43it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10041/10041 [00:00<00:00, 3646636.63it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 10459.35it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 1501182.53it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 10268.15it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 3498168.47it/s]\n"
     ]
    }
   ],
   "source": [
    "dr = DataReader('nlkt')\n",
    "train_1 = dr.read_data('data/train_text_1.txt')\n",
    "tags_train_1 = dr.read_tags('data/train_tags_1.txt')\n",
    "train_2 = dr.read_data('data/train_text_2.txt')\n",
    "tags_train_2 = dr.read_tags('data/train_tags_2.txt')\n",
    "\n",
    "test_1 = dr.read_data('data/test_text_1.txt')\n",
    "tags_test_1 = dr.read_tags('data/test_tags_1.txt')\n",
    "test_2 = dr.read_data('data/test_text_2.txt')\n",
    "tags_test_2 = dr.read_tags('data/test_tags_2.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daed333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordCounter:\n",
    "    \n",
    "    def __init__(self, data, classes, no_classes = 2):\n",
    "    \n",
    "        counter = defaultdict(int)\n",
    "        class_counter = []\n",
    "        for i in range(no_classes):\n",
    "            class_counter.append(defaultdict(int))\n",
    "        \n",
    "        wordset = []\n",
    "    \n",
    "        for tweet, tweet_class in tqdm(zip(data, classes)):\n",
    "            for word in tweet:\n",
    "                counter[word] += 1\n",
    "                class_counter[tweet_class][word] += 1\n",
    "                wordset.append(word)\n",
    "        \n",
    "        self.wordset = list(set(wordset))\n",
    "        \n",
    "        self.no_classes = no_classes\n",
    "        self.counter = counter\n",
    "        self.class_counter = class_counter\n",
    "        \n",
    "        self.save_most_dominant()\n",
    "        \n",
    "    def ask(self, word):\n",
    "        \n",
    "        total = self.counter[word]\n",
    "        classes = [self.class_counter[i][word] for i in range(self.no_classes)]\n",
    "        return (total, np.array(classes))\n",
    "    \n",
    "    def ask_distribution(self, word):\n",
    "        \n",
    "        total, over_classes = self.ask(word)\n",
    "        scaled_over_classes = np.array(over_classes, dtype = float)\n",
    "        scaled_over_classes /= float(total)\n",
    "        \n",
    "        return scaled_over_classes\n",
    "\n",
    "    def save_most_dominant(self, K = 25):\n",
    "        \n",
    "        most_dominant = []\n",
    "        for i in range(self.no_classes):\n",
    "            most_dominant.append([])\n",
    "        \n",
    "        for word in self.wordset:\n",
    "            \n",
    "            scaled_over_classes = self.ask_distribution(word)\n",
    "            \n",
    "            for i in range(self.no_classes):\n",
    "                if scaled_over_classes[i] < 0.99 and scaled_over_classes[i] > (1 / self.no_classes):\n",
    "                    most_dominant[i].append((scaled_over_classes[i], word))\n",
    "        \n",
    "        self.keywords = []\n",
    "        \n",
    "        for i in range(self.no_classes):\n",
    "            most_dominant[i] = sorted(most_dominant[i], reverse = True)\n",
    "            if len(most_dominant[i]) > K:\n",
    "                most_dominant[i] = most_dominant[i][:K]\n",
    "            print(most_dominant[i])\n",
    "            \n",
    "            self.keywords.append([])\n",
    "            for frac, word in most_dominant[i]:\n",
    "                self.keywords[i].append(word)\n",
    "            \n",
    "            print(self.keywords[i])\n",
    "    \n",
    "    def tweet_class_distribution(self, tweet):\n",
    "        \n",
    "        \n",
    "        res = np.ones(self.no_classes)\n",
    "        res[0] = 1.01\n",
    "        \n",
    "        my_keywords = []\n",
    "        for i in range(1, self.no_classes):\n",
    "            my_keywords += self.keywords[i]\n",
    "        \n",
    "        my_keywords = list(set(my_keywords))\n",
    "        \n",
    "        \n",
    "        for word in tweet:\n",
    "            #print(word)\n",
    "            if word in my_keywords:\n",
    "                \n",
    "                weights = 1 + self.ask_distribution(word)\n",
    "                res *= weights\n",
    "        \n",
    "        return res\n",
    "        \n",
    "            \n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f51ca10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10041it [00:00, 256445.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9893617021276596, 'chodzi'), (0.9886363636363636, 'p'), (0.9876543209876543, 'je≈õli'), (0.9864864864864865, 'wis≈Çy'), (0.9857142857142858, 'wtedy'), (0.9857142857142858, 'mamy'), (0.9856115107913669, 'jestem'), (0.9847328244274809, 'mam'), (0.9831932773109243, 'by≈Ço'), (0.9818181818181818, 'jednak'), (0.9807692307692307, 'zaraz'), (0.9807692307692307, 'temu'), (0.9807692307692307, 'oczywi≈õcie'), (0.9803921568627451, 'sumie'), (0.9803921568627451, 'bƒôdƒô'), (0.9803779069767442, ':'), (0.98, 'jakie'), (0.98, 'dobry'), (0.9795918367346939, 'xd'), (0.9795918367346939, 'pracy'), (0.9795918367346939, 'gra≈Ç'), (0.9787234042553191, 'skoro'), (0.9787234042553191, 'razie'), (0.9777777777777777, 'wszystkich'), (0.9777777777777777, 'mln')]\n",
      "['chodzi', 'p', 'je≈õli', 'wis≈Çy', 'wtedy', 'mamy', 'jestem', 'mam', 'by≈Ço', 'jednak', 'zaraz', 'temu', 'oczywi≈õcie', 'sumie', 'bƒôdƒô', ':', 'jakie', 'dobry', 'xd', 'pracy', 'gra≈Ç', 'skoro', 'razie', 'wszystkich', 'mln']\n",
      "[(0.8571428571428571, 'pisowska'), (0.8571428571428571, 'k≈Çamco'), (0.8333333333333334, 'parchu'), (0.8333333333333334, 'kutasie'), (0.8, '≈õmierdzi'), (0.8, 'zjebie'), (0.8, 'zdrajcy'), (0.8, 'zamknij'), (0.8, 'piskomuna'), (0.8, 'pisdzielstwa'), (0.8, 'pedofili'), (0.8, 'niemiecki'), (0.8, 'id≈∫cie'), (0.75, '≈õmierdzƒÖcƒÖ'), (0.75, 'z≈Çodzieju'), (0.75, 'wz√≥r'), (0.75, 'tƒôpy'), (0.75, 'temperatury'), (0.75, 'skurwysyny'), (0.75, 'pad≈Ç'), (0.75, 'napiszƒô'), (0.75, 'mocniej'), (0.75, 'kutasa'), (0.75, 'kurwy'), (0.75, 'komuchami')]\n",
      "['pisowska', 'k≈Çamco', 'parchu', 'kutasie', '≈õmierdzi', 'zjebie', 'zdrajcy', 'zamknij', 'piskomuna', 'pisdzielstwa', 'pedofili', 'niemiecki', 'id≈∫cie', '≈õmierdzƒÖcƒÖ', 'z≈Çodzieju', 'wz√≥r', 'tƒôpy', 'temperatury', 'skurwysyny', 'pad≈Ç', 'napiszƒô', 'mocniej', 'kutasa', 'kurwy', 'komuchami']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10041it [00:00, 219422.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9893617021276596, 'chodzi'), (0.9886363636363636, 'p'), (0.9876543209876543, 'je≈õli'), (0.9864864864864865, 'wis≈Çy'), (0.9857142857142858, 'wtedy'), (0.9857142857142858, 'mamy'), (0.9856115107913669, 'jestem'), (0.9847328244274809, 'mam'), (0.9831932773109243, 'by≈Ço'), (0.9818181818181818, 'jednak'), (0.9807692307692307, 'zaraz'), (0.9807692307692307, 'temu'), (0.9807692307692307, 'oczywi≈õcie'), (0.9803921568627451, 'sumie'), (0.9803921568627451, 'bƒôdƒô'), (0.9803779069767442, ':'), (0.98, 'jakie'), (0.98, 'dobry'), (0.9795918367346939, 'xd'), (0.9795918367346939, 'pracy'), (0.9795918367346939, 'gra≈Ç'), (0.9787234042553191, 'skoro'), (0.9787234042553191, 'razie'), (0.9777777777777777, 'wszystkich'), (0.9777777777777777, 'mln')]\n",
      "['chodzi', 'p', 'je≈õli', 'wis≈Çy', 'wtedy', 'mamy', 'jestem', 'mam', 'by≈Ço', 'jednak', 'zaraz', 'temu', 'oczywi≈õcie', 'sumie', 'bƒôdƒô', ':', 'jakie', 'dobry', 'xd', 'pracy', 'gra≈Ç', 'skoro', 'razie', 'wszystkich', 'mln']\n",
      "[(0.75, 'wz√≥r'), (0.6666666666666666, 'zajmij'), (0.6666666666666666, 'wywaliƒá'), (0.6666666666666666, 'wazeliny'), (0.6666666666666666, 'sieroto'), (0.6666666666666666, 'sierot'), (0.6666666666666666, 'putina'), (0.6666666666666666, 'przyjaciele'), (0.6666666666666666, 'krystyny'), (0.6666666666666666, 'kontem'), (0.6666666666666666, 'karakanica'), (0.6666666666666666, 'kaczorzyca'), (0.6666666666666666, 'jadowita'), (0.6666666666666666, 'hi'), (0.6666666666666666, 'genetyczny'), (0.6666666666666666, 'cwelica'), (0.6666666666666666, 'chuju'), (0.6666666666666666, 'biedaku'), (0.6666666666666666, 'alfonsa'), (0.6, 'zjebie'), (0.6, 'twoi'), (0.5, '≈ºulem'), (0.5, '≈ºerujƒÖ'), (0.5, '≈õwiƒôto'), (0.5, '≈õmiecie')]\n",
      "['wz√≥r', 'zajmij', 'wywaliƒá', 'wazeliny', 'sieroto', 'sierot', 'putina', 'przyjaciele', 'krystyny', 'kontem', 'karakanica', 'kaczorzyca', 'jadowita', 'hi', 'genetyczny', 'cwelica', 'chuju', 'biedaku', 'alfonsa', 'zjebie', 'twoi', '≈ºulem', '≈ºerujƒÖ', '≈õwiƒôto', '≈õmiecie']\n",
      "[(0.8571428571428571, 'k≈Çamco'), (0.8333333333333334, 'bura'), (0.8, '≈Ç≈ºesz'), (0.8, 'pisdzielstwa'), (0.8, 'niemiecki'), (0.8, 'id≈∫cie'), (0.75, '≈õmierdzƒÖcƒÖ'), (0.75, 'z≈Çodzieju'), (0.75, 'tƒôpy'), (0.75, 'skurwysyny'), (0.75, 'oszu≈õcie'), (0.75, 'mocniej'), (0.75, 'kutasa'), (0.75, 'inteligentna'), (0.75, 'gnoju'), (0.75, 'chuje'), (0.6666666666666666, 'ü§™'), (0.6666666666666666, '≈ºywych'), (0.6666666666666666, '≈ºywy'), (0.6666666666666666, '≈õmietnik'), (0.6666666666666666, 'z≈Çodziejstwo'), (0.6666666666666666, 'waszego'), (0.6666666666666666, 'ustawa'), (0.6666666666666666, 'udaje'), (0.6666666666666666, 'typa')]\n",
      "['k≈Çamco', 'bura', '≈Ç≈ºesz', 'pisdzielstwa', 'niemiecki', 'id≈∫cie', '≈õmierdzƒÖcƒÖ', 'z≈Çodzieju', 'tƒôpy', 'skurwysyny', 'oszu≈õcie', 'mocniej', 'kutasa', 'inteligentna', 'gnoju', 'chuje', 'ü§™', '≈ºywych', '≈ºywy', '≈õmietnik', 'z≈Çodziejstwo', 'waszego', 'ustawa', 'udaje', 'typa']\n"
     ]
    }
   ],
   "source": [
    "wc1 = WordCounter(train_1, tags_train_1, no_classes = 2)\n",
    "wc2 = WordCounter(train_2, tags_train_2, no_classes = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a1e7f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_classifier_1 = AbstractClassifier(wc1)\n",
    "wc_classifier_2 = AbstractClassifier(wc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8157ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_classifier_1.run_and_save(test_1, 'results/binary_word_counter.txt')\n",
    "wc_classifier_2.run_and_save(test_2, 'results/ternary_word_counter.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e2ee766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self, word_counter):\n",
    "\n",
    "        \n",
    "        self.no_classes = word_counter.no_classes\n",
    "        self.wordset = word_counter.wordset\n",
    "        \n",
    "        counts = []\n",
    "        \n",
    "        for word in word_counter.wordset:\n",
    "            counts.append(word_counter.ask(word)[1])\n",
    "        \n",
    "        counts = np.array(counts)\n",
    "        \n",
    "        self.df = pd.DataFrame(index = word_counter.wordset,\n",
    "                          columns = np.arange(word_counter.no_classes), data = counts)\n",
    "        \n",
    "        self.df /= self.df.sum(0)\n",
    "        \n",
    "        self.log_df = np.log(1e-100 + self.df)\n",
    "    \n",
    "    def tweet_class_distribution(self, tweet):\n",
    "        \n",
    "        log_probs = np.zeros(self.no_classes)\n",
    "    \n",
    "        apriori_prob = -np.log(self.no_classes)\n",
    "        prob_d = 0\n",
    "\n",
    "        for cur_class in range(self.no_classes):\n",
    "            cur_prob = apriori_prob\n",
    "            for word in tweet:\n",
    "                if word not in self.wordset:\n",
    "                    continue\n",
    "                cur_prob += (self.log_df.loc[word, cur_class])\n",
    "            log_probs[cur_class] = cur_prob\n",
    "            prob_d += np.exp(cur_prob)\n",
    "    \n",
    "        prob_d = np.log(prob_d)\n",
    "    \n",
    "        probs = np.zeros(self.no_classes)\n",
    "        \n",
    "        for cur_class in range(self.no_classes):\n",
    "            p = log_probs[cur_class] - prob_d\n",
    "            probs[cur_class] = np.exp(p)\n",
    "        \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd62944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb1 = NaiveBayes(wc1)\n",
    "nb2 = NaiveBayes(wc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d1775e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier_1 = AbstractClassifier(nb1)\n",
    "nb_classifier_2 = AbstractClassifier(nb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c86e1913",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier_1.run_and_save(test_1, 'results/binary_naive_bayes.txt')\n",
    "nb_classifier_2.run_and_save(test_2, 'results/ternary_naive_bayes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb669ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes\n",
      "Precision = 41.57%\n",
      "Recall = 27.61%\n",
      "Balanced F-score = 33.18%\n",
      "Accuracy = 85.10%\n",
      "\n",
      "\n",
      "simple network\n",
      "Precision = 41.58%\n",
      "Recall = 31.34%\n",
      "Balanced F-score = 35.74%\n",
      "Accuracy = 84.90%\n",
      "\n",
      "\n",
      "word counter\n",
      "Precision = 85.71%\n",
      "Recall = 4.48%\n",
      "Balanced F-score = 8.51%\n",
      "Accuracy = 87.10%\n"
     ]
    }
   ],
   "source": [
    "print(\"naive bayes\")\n",
    "!perl graders/evaluate1.pl results/binary_naive_bayes.txt\n",
    "print(\"\\n\\nsimple network\")\n",
    "!perl graders/evaluate1.pl results/binary_simple_network.txt\n",
    "print(\"\\n\\nword counter\")\n",
    "!perl graders/evaluate1.pl results/binary_word_counter.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "737a2449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes\n",
      "Micro-Average F-score = 84.20%\n",
      "Macro-Average F-score = 44.54%\n",
      "\n",
      "\n",
      "simple network\n",
      "Micro-Average F-score = 86.80%\n",
      "Macro-Average F-score = 47.09%\n",
      "\n",
      "\n",
      "word counter\n",
      "Micro-Average F-score = 87.50%\n",
      "Macro-Average F-score = 53.87%\n"
     ]
    }
   ],
   "source": [
    "print(\"naive bayes\")\n",
    "!perl graders/evaluate2.pl results/ternary_naive_bayes.txt\n",
    "print(\"\\n\\nsimple network\")\n",
    "!perl graders/evaluate2.pl results/ternary_simple_network.txt\n",
    "print(\"\\n\\nword counter\")\n",
    "!perl graders/evaluate2.pl results/ternary_word_counter.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb419643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44929681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f717bfc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b8cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e61dd4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
