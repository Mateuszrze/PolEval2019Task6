{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a87bcc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize as nlkt_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07efb321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_specials(text):\n",
    "    \n",
    "    specials = {\n",
    "        '\\u0026gt;' : '>',\n",
    "        '\\u0026lt;' : '<',\n",
    "        '\\u0026amp;' : '&',\n",
    "        '\\\\n' : ' ',\n",
    "        '\\\\r' : ' ',\n",
    "        '\\\\\"' : '\"'\n",
    "        \n",
    "    }\n",
    "    \n",
    "    for special in specials.keys():\n",
    "        text = text.replace(special, specials[special])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8e33904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_tokenize(text):\n",
    "    return text.split(' ')\n",
    "\n",
    "def tokenize_with_nlkt(text):\n",
    "    \n",
    "    text_clean = replace_specials(text)\n",
    "    text_lowered = text_clean.lower()\n",
    "    tokenized = nlkt_tokenize(text_lowered)\n",
    "    \n",
    "    return tokenized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f987019",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "    \n",
    "    def __init__(self, default_tokenize):\n",
    "        \n",
    "        self.default_tokenize = default_tokenize\n",
    "    \n",
    "    def read_data(self, filepath, tokenize = None):\n",
    "        \n",
    "        if tokenize is None:\n",
    "            tokenize = self.default_tokenize\n",
    "        \n",
    "        data = []\n",
    "        \n",
    "        my_file = open(filepath, 'r')\n",
    "        for line in tqdm(my_file.readlines()):\n",
    "            data.append(tokenize(line))\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def read_tags(self, filepath):\n",
    "        \n",
    "        tags = []\n",
    "        \n",
    "        my_file = open(filepath, 'r')\n",
    "        for line in tqdm(my_file.readlines()):\n",
    "            tags.append(int(line))\n",
    "        \n",
    "        return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "897ed856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 10041/10041 [00:00<00:00, 11044.23it/s]\n",
      "100%|████████████████████████████████| 10041/10041 [00:00<00:00, 3557010.68it/s]\n",
      "100%|██████████████████████████████████| 10041/10041 [00:00<00:00, 11535.13it/s]\n",
      "100%|████████████████████████████████| 10041/10041 [00:00<00:00, 4290444.83it/s]\n",
      "100%|████████████████████████████████████| 1000/1000 [00:00<00:00, 10890.42it/s]\n",
      "100%|██████████████████████████████████| 1000/1000 [00:00<00:00, 3572660.99it/s]\n",
      "100%|████████████████████████████████████| 1000/1000 [00:00<00:00, 10979.91it/s]\n",
      "100%|██████████████████████████████████| 1000/1000 [00:00<00:00, 3350083.07it/s]\n"
     ]
    }
   ],
   "source": [
    "dr = DataReader(tokenize_with_nlkt)\n",
    "train_1 = dr.read_data('data/train_text_1.txt')\n",
    "tags_train_1 = dr.read_tags('data/train_tags_1.txt')\n",
    "train_2 = dr.read_data('data/train_text_2.txt')\n",
    "tags_train_2 = dr.read_tags('data/train_tags_2.txt')\n",
    "\n",
    "test_1 = dr.read_data('data/test_text_1.txt')\n",
    "tags_test_1 = dr.read_tags('data/test_tags_1.txt')\n",
    "test_2 = dr.read_data('data/test_text_2.txt')\n",
    "tags_test_2 = dr.read_tags('data/test_tags_2.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ae75ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractClassifier:\n",
    "    \n",
    "    def __init__(self, classifier):\n",
    "        \n",
    "        self.classifier = classifier\n",
    "    \n",
    "    def classify(self, tweet):\n",
    "        \n",
    "        return np.argmax(self.classifier.tweet_class_distribution(tweet))\n",
    "            \n",
    "    def classify_many(self, tweets):\n",
    "        \n",
    "        answer = [self.classify(tweet) for tweet in tweets]\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def run_and_save(self, tweets, filename):\n",
    "        \n",
    "        answer = self.classify_many(tweets)\n",
    "        \n",
    "        f = open(filename, 'w+')\n",
    "        for ans in answer:\n",
    "            f.write(str(ans) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9d652d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordCounter:\n",
    "    \n",
    "    def __init__(self, data, classes, no_classes = 2):\n",
    "    \n",
    "        counter = defaultdict(int)\n",
    "        class_counter = []\n",
    "        for i in range(no_classes):\n",
    "            class_counter.append(defaultdict(int))\n",
    "        \n",
    "        wordset = []\n",
    "    \n",
    "        for tweet, tweet_class in tqdm(zip(data, classes)):\n",
    "            for word in tweet:\n",
    "                counter[word] += 1\n",
    "                class_counter[tweet_class][word] += 1\n",
    "                wordset.append(word)\n",
    "        \n",
    "        self.wordset = list(set(wordset))\n",
    "        \n",
    "        self.no_classes = no_classes\n",
    "        self.counter = counter\n",
    "        self.class_counter = class_counter\n",
    "        \n",
    "        self.save_most_dominant()\n",
    "        \n",
    "    def ask(self, word):\n",
    "        \n",
    "        total = self.counter[word]\n",
    "        classes = [self.class_counter[i][word] for i in range(self.no_classes)]\n",
    "        return (total, np.array(classes))\n",
    "    \n",
    "    def ask_distribution(self, word):\n",
    "        \n",
    "        total, over_classes = self.ask(word)\n",
    "        scaled_over_classes = np.array(over_classes, dtype = float)\n",
    "        scaled_over_classes /= float(total)\n",
    "        \n",
    "        return scaled_over_classes\n",
    "\n",
    "    def save_most_dominant(self, K = 25):\n",
    "        \n",
    "        most_dominant = []\n",
    "        for i in range(self.no_classes):\n",
    "            most_dominant.append([])\n",
    "        \n",
    "        for word in self.wordset:\n",
    "            \n",
    "            scaled_over_classes = self.ask_distribution(word)\n",
    "            \n",
    "            for i in range(self.no_classes):\n",
    "                if scaled_over_classes[i] < 0.99 and scaled_over_classes[i] > (1 / self.no_classes):\n",
    "                    most_dominant[i].append((scaled_over_classes[i], word))\n",
    "        \n",
    "        self.keywords = []\n",
    "        \n",
    "        for i in range(self.no_classes):\n",
    "            most_dominant[i] = sorted(most_dominant[i], reverse = True)\n",
    "            if len(most_dominant[i]) > K:\n",
    "                most_dominant[i] = most_dominant[i][:K]\n",
    "            print(most_dominant[i])\n",
    "            \n",
    "            self.keywords.append([])\n",
    "            for frac, word in most_dominant[i]:\n",
    "                self.keywords[i].append(word)\n",
    "            \n",
    "            print(self.keywords[i])\n",
    "    \n",
    "    def tweet_class_distribution(self, tweet):\n",
    "        \n",
    "        \n",
    "        res = np.ones(self.no_classes)\n",
    "        res[0] = 1.01\n",
    "        \n",
    "        my_keywords = []\n",
    "        for i in range(1, self.no_classes):\n",
    "            my_keywords += self.keywords[i]\n",
    "        \n",
    "        my_keywords = list(set(my_keywords))\n",
    "        \n",
    "        \n",
    "        for word in tweet:\n",
    "            #print(word)\n",
    "            if word in my_keywords:\n",
    "                \n",
    "                weights = 1 + self.ask_distribution(word)\n",
    "                res *= weights\n",
    "        \n",
    "        return res\n",
    "        \n",
    "            \n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0bd32c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10041it [00:00, 249743.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9893617021276596, 'chodzi'), (0.9886363636363636, 'p'), (0.9876543209876543, 'jeśli'), (0.9864864864864865, 'wisły'), (0.9857142857142858, 'wtedy'), (0.9857142857142858, 'mamy'), (0.9856115107913669, 'jestem'), (0.9847328244274809, 'mam'), (0.9831932773109243, 'było'), (0.9818181818181818, 'jednak'), (0.9807692307692307, 'zaraz'), (0.9807692307692307, 'temu'), (0.9807692307692307, 'oczywiście'), (0.9803921568627451, 'sumie'), (0.9803921568627451, 'będę'), (0.9803779069767442, ':'), (0.98, 'jakie'), (0.98, 'dobry'), (0.9795918367346939, 'xd'), (0.9795918367346939, 'pracy'), (0.9795918367346939, 'grał'), (0.9787234042553191, 'skoro'), (0.9787234042553191, 'razie'), (0.9777777777777777, 'wszystkich'), (0.9777777777777777, 'mln')]\n",
      "['chodzi', 'p', 'jeśli', 'wisły', 'wtedy', 'mamy', 'jestem', 'mam', 'było', 'jednak', 'zaraz', 'temu', 'oczywiście', 'sumie', 'będę', ':', 'jakie', 'dobry', 'xd', 'pracy', 'grał', 'skoro', 'razie', 'wszystkich', 'mln']\n",
      "[(0.8571428571428571, 'pisowska'), (0.8571428571428571, 'kłamco'), (0.8333333333333334, 'parchu'), (0.8333333333333334, 'kutasie'), (0.8, 'śmierdzi'), (0.8, 'zjebie'), (0.8, 'zdrajcy'), (0.8, 'zamknij'), (0.8, 'piskomuna'), (0.8, 'pisdzielstwa'), (0.8, 'pedofili'), (0.8, 'niemiecki'), (0.8, 'idźcie'), (0.75, 'śmierdzącą'), (0.75, 'złodzieju'), (0.75, 'wzór'), (0.75, 'tępy'), (0.75, 'temperatury'), (0.75, 'skurwysyny'), (0.75, 'padł'), (0.75, 'napiszę'), (0.75, 'mocniej'), (0.75, 'kutasa'), (0.75, 'kurwy'), (0.75, 'komuchami')]\n",
      "['pisowska', 'kłamco', 'parchu', 'kutasie', 'śmierdzi', 'zjebie', 'zdrajcy', 'zamknij', 'piskomuna', 'pisdzielstwa', 'pedofili', 'niemiecki', 'idźcie', 'śmierdzącą', 'złodzieju', 'wzór', 'tępy', 'temperatury', 'skurwysyny', 'padł', 'napiszę', 'mocniej', 'kutasa', 'kurwy', 'komuchami']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10041it [00:00, 251724.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9893617021276596, 'chodzi'), (0.9886363636363636, 'p'), (0.9876543209876543, 'jeśli'), (0.9864864864864865, 'wisły'), (0.9857142857142858, 'wtedy'), (0.9857142857142858, 'mamy'), (0.9856115107913669, 'jestem'), (0.9847328244274809, 'mam'), (0.9831932773109243, 'było'), (0.9818181818181818, 'jednak'), (0.9807692307692307, 'zaraz'), (0.9807692307692307, 'temu'), (0.9807692307692307, 'oczywiście'), (0.9803921568627451, 'sumie'), (0.9803921568627451, 'będę'), (0.9803779069767442, ':'), (0.98, 'jakie'), (0.98, 'dobry'), (0.9795918367346939, 'xd'), (0.9795918367346939, 'pracy'), (0.9795918367346939, 'grał'), (0.9787234042553191, 'skoro'), (0.9787234042553191, 'razie'), (0.9777777777777777, 'wszystkich'), (0.9777777777777777, 'mln')]\n",
      "['chodzi', 'p', 'jeśli', 'wisły', 'wtedy', 'mamy', 'jestem', 'mam', 'było', 'jednak', 'zaraz', 'temu', 'oczywiście', 'sumie', 'będę', ':', 'jakie', 'dobry', 'xd', 'pracy', 'grał', 'skoro', 'razie', 'wszystkich', 'mln']\n",
      "[(0.75, 'wzór'), (0.6666666666666666, 'zajmij'), (0.6666666666666666, 'wywalić'), (0.6666666666666666, 'wazeliny'), (0.6666666666666666, 'sieroto'), (0.6666666666666666, 'sierot'), (0.6666666666666666, 'putina'), (0.6666666666666666, 'przyjaciele'), (0.6666666666666666, 'krystyny'), (0.6666666666666666, 'kontem'), (0.6666666666666666, 'karakanica'), (0.6666666666666666, 'kaczorzyca'), (0.6666666666666666, 'jadowita'), (0.6666666666666666, 'hi'), (0.6666666666666666, 'genetyczny'), (0.6666666666666666, 'cwelica'), (0.6666666666666666, 'chuju'), (0.6666666666666666, 'biedaku'), (0.6666666666666666, 'alfonsa'), (0.6, 'zjebie'), (0.6, 'twoi'), (0.5, 'żulem'), (0.5, 'żerują'), (0.5, 'święto'), (0.5, 'śmiecie')]\n",
      "['wzór', 'zajmij', 'wywalić', 'wazeliny', 'sieroto', 'sierot', 'putina', 'przyjaciele', 'krystyny', 'kontem', 'karakanica', 'kaczorzyca', 'jadowita', 'hi', 'genetyczny', 'cwelica', 'chuju', 'biedaku', 'alfonsa', 'zjebie', 'twoi', 'żulem', 'żerują', 'święto', 'śmiecie']\n",
      "[(0.8571428571428571, 'kłamco'), (0.8333333333333334, 'bura'), (0.8, 'łżesz'), (0.8, 'pisdzielstwa'), (0.8, 'niemiecki'), (0.8, 'idźcie'), (0.75, 'śmierdzącą'), (0.75, 'złodzieju'), (0.75, 'tępy'), (0.75, 'skurwysyny'), (0.75, 'oszuście'), (0.75, 'mocniej'), (0.75, 'kutasa'), (0.75, 'inteligentna'), (0.75, 'gnoju'), (0.75, 'chuje'), (0.6666666666666666, '🤪'), (0.6666666666666666, 'żywych'), (0.6666666666666666, 'żywy'), (0.6666666666666666, 'śmietnik'), (0.6666666666666666, 'złodziejstwo'), (0.6666666666666666, 'waszego'), (0.6666666666666666, 'ustawa'), (0.6666666666666666, 'udaje'), (0.6666666666666666, 'typa')]\n",
      "['kłamco', 'bura', 'łżesz', 'pisdzielstwa', 'niemiecki', 'idźcie', 'śmierdzącą', 'złodzieju', 'tępy', 'skurwysyny', 'oszuście', 'mocniej', 'kutasa', 'inteligentna', 'gnoju', 'chuje', '🤪', 'żywych', 'żywy', 'śmietnik', 'złodziejstwo', 'waszego', 'ustawa', 'udaje', 'typa']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wc1 = WordCounter(train_1, tags_train_1, no_classes = 2)\n",
    "wc2 = WordCounter(train_2, tags_train_2, no_classes = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9c0603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_classifier_1 = AbstractClassifier(wc1)\n",
    "wc_classifier_2 = AbstractClassifier(wc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbb37000",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_classifier_1.run_and_save(test_1, 'data/answers1wc.txt')\n",
    "wc_classifier_2.run_and_save(test_2, 'data/answers2wc.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae030ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self, word_counter):\n",
    "\n",
    "        \n",
    "        self.no_classes = word_counter.no_classes\n",
    "        self.wordset = word_counter.wordset\n",
    "        \n",
    "        counts = []\n",
    "        \n",
    "        for word in word_counter.wordset:\n",
    "            counts.append(word_counter.ask(word)[1])\n",
    "        \n",
    "        counts = np.array(counts)\n",
    "        \n",
    "        self.df = pd.DataFrame(index = word_counter.wordset,\n",
    "                          columns = np.arange(word_counter.no_classes), data = counts)\n",
    "        \n",
    "        self.df /= self.df.sum(0)\n",
    "        \n",
    "        self.log_df = np.log(1e-100 + self.df)\n",
    "    \n",
    "    def tweet_class_distribution(self, tweet):\n",
    "        \n",
    "        log_probs = np.zeros(self.no_classes)\n",
    "    \n",
    "        apriori_prob = -np.log(self.no_classes)\n",
    "        prob_d = 0\n",
    "\n",
    "        for cur_class in range(self.no_classes):\n",
    "            cur_prob = apriori_prob\n",
    "            for word in tweet:\n",
    "                if word not in self.wordset:\n",
    "                    continue\n",
    "                cur_prob += (self.log_df.loc[word, cur_class])\n",
    "            log_probs[cur_class] = cur_prob\n",
    "            prob_d += np.exp(cur_prob)\n",
    "    \n",
    "        prob_d = np.log(prob_d)\n",
    "    \n",
    "        probs = np.zeros(self.no_classes)\n",
    "        \n",
    "        for cur_class in range(self.no_classes):\n",
    "            p = log_probs[cur_class] - prob_d\n",
    "            probs[cur_class] = np.exp(p)\n",
    "        \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dceff29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb1 = NaiveBayes(wc1)\n",
    "nb2 = NaiveBayes(wc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc80c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier_1 = AbstractClassifier(nb1)\n",
    "nb_classifier_2 = AbstractClassifier(nb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e16942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier_1.run_and_save(test_1, 'data/answers1nb.txt')\n",
    "nb_classifier_2.run_and_save(test_2, 'data/answers2nb.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "620f45cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot open! at data/evaluate1.pl line 13.\r\n"
     ]
    }
   ],
   "source": [
    "! perl data/evaluate1.pl answers1wc.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfe9cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
