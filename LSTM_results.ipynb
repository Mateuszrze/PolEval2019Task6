{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "183d3988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_reader import DataReader\n",
    "import datasets\n",
    "from tweet_to_vec import TweetToVec\n",
    "import utils\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from LSTM import LSTMModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e161fe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'fixed_length_2d'\n",
    "L = 12\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78283e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10041 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10041/10041 [00:01<00:00, 5483.16it/s]\n",
      "100%|██████████| 10041/10041 [00:00<00:00, 2267783.45it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4889.96it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1748355.15it/s]\n",
      "100%|██████████| 10041/10041 [00:01<00:00, 5384.59it/s]\n",
      "100%|██████████| 10041/10041 [00:00<00:00, 2241352.13it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 5767.02it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 906484.55it/s]\n",
      "100%|██████████| 20067/20067 [00:00<00:00, 35771.45it/s]\n"
     ]
    }
   ],
   "source": [
    "dr = DataReader('nlkt')\n",
    "dr.read_dataset(datasets.binary_classes)\n",
    "dr.read_dataset(datasets.ternary_classes)\n",
    "embeddings = dr.read_embeddings('embeddings/kraby.txt')\n",
    "t2v = TweetToVec(embeddings, method, L)\n",
    "batched_binary = t2v.vectorize_and_batch_dataset(dr.get_dataset('binary'), batch_size, True)\n",
    "batched_trenary = t2v.vectorize_and_batch_dataset(dr.get_dataset('ternary'), batch_size, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad0b3502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first comment to the model: it learned that returning 0 is a good solution and kept doing that all the time. \n",
    "# Thus, we altered the dataset to contain the same amount of results for 0, 1 and 2\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, size = 32, embedding_dim = 100, num_layers = 3, number_of_output_classes = 2, device='cpu'):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm_size = size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.fc = nn.Sequential(nn.Linear(self.lstm_size, number_of_output_classes))\n",
    "        self.init_params()\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        output, state = self.lstm(x, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device))\n",
    "\n",
    "    def init_params(self):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for name, p in self.named_parameters():\n",
    "                if \"weight\" in name:\n",
    "                    p.normal_(0, np.sqrt(1 / (2 * p.size(dim = 1))))\n",
    "                elif \"bias\" in name:\n",
    "                    p.zero_()\n",
    "                    \n",
    "    def predict(self, x):\n",
    "        state_h, state_c = self.init_state(L)\n",
    "        predictions, (_, _) = self(x, (state_h, state_c))\n",
    "        return torch.argmax(predictions[:, -1, :], dim = 1)\n",
    "    \n",
    "    def train(self, training_data, training_classes, epochs):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.0001)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            tot_loss = 0\n",
    "            losses = []\n",
    "            for (input_batch, true_classes) in zip(training_data, training_classes):\n",
    "                optimizer.zero_grad()\n",
    "                state_h, state_c = self.init_state(L)\n",
    "                y_pred, (state_h, state_c) = self(input_batch, (state_h, state_c))\n",
    "                loss = criterion(y_pred[:, -1, :], true_classes)\n",
    "                tot_loss += loss\n",
    "                losses.append(float(loss))\n",
    "\n",
    "                state_h = state_h.detach()\n",
    "                state_c = state_c.detach()            \n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print(f'After epoch {epoch} tot_loss = {tot_loss}')\n",
    "            # scheduler.step()\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5e84c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:12<05:57, 12.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 0 tot_loss = 427.68035888671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:25<05:52, 12.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 1 tot_loss = 388.9908142089844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:38<05:45, 12.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 2 tot_loss = 376.66302490234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:51<05:41, 13.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 3 tot_loss = 320.18218994140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [01:05<05:32, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 4 tot_loss = 244.76834106445312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [01:19<05:22, 13.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 5 tot_loss = 190.25254821777344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [01:32<05:06, 13.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 6 tot_loss = 160.84951782226562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [01:44<04:48, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 7 tot_loss = 142.71217346191406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [01:57<04:35, 13.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 8 tot_loss = 123.35386657714844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [02:10<04:20, 13.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 9 tot_loss = 115.07255554199219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [02:24<04:08, 13.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 10 tot_loss = 107.65995025634766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [02:37<03:55, 13.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 11 tot_loss = 101.9082260131836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [02:44<04:06, 13.71s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-00120df42657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_binary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training tweets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched_binary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training tags'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-cba2b7064737>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_data, training_classes, epochs)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "model = LSTMModel()\n",
    "model.to(device)\n",
    "model.train(batched_binary['training tweets'], batched_binary['training tags'], 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e1c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_predictions = model.predict(batched_binary['test tweets'])\n",
    "utils.save_results(binary_predictions, 'results/binary_LSTM.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76892a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 25.00%\n",
      "Recall = 0.75%\n",
      "Balanced F-score = 1.45%\n",
      "Accuracy = 86.40%\n"
     ]
    }
   ],
   "source": [
    "!perl graders/evaluate1.pl results/binary_LSTM.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbc912f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
